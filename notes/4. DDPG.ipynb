{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Determenistic Policy Gradients\n",
    "Work in progress nut it will be finished soon enough\n",
    "\n",
    "Special thanks to KnightofK9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "cuda = torch.device('cpu')\n",
    "frame_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../data/ml-20m/ratings.csv')\n",
    "movies = pickle.load(open('../data/infos_pca128.pytorch', 'rb'))\n",
    "infos_web = json.load(open('../data/infos.json')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: KnightofK9\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda i: 2 * (i - 2.5))\n",
    "users = ratings[[\"userId\",\"movieId\"]].groupby([\"userId\"]).size()\n",
    "users = users[users >= frame_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = users[-100:]\n",
    "train_users = users[:-100]\n",
    "\n",
    "train_ratings = ratings[ratings[\"userId\"].isin(train_users.index)]\n",
    "test_ratings = ratings[ratings[\"userId\"].isin(test_users.index)]\n",
    "\n",
    "train_ratings = train_ratings.sort_values(by=[\"userId\", \"timestamp\"]).drop(columns=[\"timestamp\"]).set_index(\"userId\")\n",
    "test_ratings = test_ratings.sort_values(by=[\"userId\", \"timestamp\"]).drop(columns=[\"timestamp\"]).set_index(\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in movies.keys():\n",
    "    movies[i] = movies[i].to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateRepresentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateRepresentation, self).__init__()\n",
    "        self.lin = nn.Sequential(\n",
    "            # 128 - embed size, 1 - rating size\n",
    "            nn.Linear(frame_size * (128 + 1), 256),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, info, ratings):\n",
    "        # raw_size - size of the raw movie info. Constant = 2591\n",
    "        # embed_size - size of an ebedded movie. Constant = 128\n",
    "        # raw -> embed via embeddings module defined above\n",
    "        # input: currently info is batch_size x frame_size x raw_size\n",
    "        # step 1: tramsform info to batch_size x (frame_size * embed_size)\n",
    "        info = info.view(batch_size, frame_size * 128)\n",
    "        # step 2: stack info with ratings. stacked: batch_size x (embed_size + 1)\n",
    "        stacked = torch.cat([info, ratings], 1)\n",
    "        # step 3: apply state represemtation module\n",
    "        state = self.lin(stacked)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.state_rep = StateRepresentation()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, info, rewards):\n",
    "        state = self.state_rep(info, rewards)\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        # x = torch.tanh(self.linear3(x)) # in case embeds are -1 1 normalized\n",
    "        x = self.linear3(x) # in case embeds are standard scaled / wiped using PCA whitening\n",
    "        return state, x\n",
    "    \n",
    "    def get_action(self, info, rewards):\n",
    "        state, action = self.forward(info, rewards)\n",
    "        return state, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        action = torch.squeeze(action)\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Code\n",
    "[Creditals](https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(batch, \n",
    "           gamma = 0.99,\n",
    "           min_value=-5,\n",
    "           max_value=5,\n",
    "           soft_tau=1e-2):\n",
    "    \n",
    "    state, action, reward, next_state, done = batch\n",
    "    \n",
    "    reward     = reward.unsqueeze(1)\n",
    "    done       = done.unsqueeze(1)\n",
    "    \n",
    "    enc_state, current_action = policy_net(*state)\n",
    "    \n",
    "    current_action_var = np.var(current_action.detach().cpu().numpy()).mean()\n",
    "    current_action_std = np.std(current_action.detach().cpu().numpy()).mean()\n",
    "    current_action_mean = np.mean(current_action.detach().cpu().numpy()).mean()\n",
    "    current_action_cov = np.cov(current_action.detach().cpu().numpy())\n",
    "    current_action_emb = torch.cat((current_action[0].unsqueeze(0), state[0][0]))\n",
    "\n",
    "    policy_loss = value_net(enc_state, current_action)\n",
    "    policy_loss = -policy_loss.mean()\n",
    "    policy_loss = torch.clamp(policy_loss, min_value, max_value)\n",
    "\n",
    "    enc_next_state, next_action = target_policy_net(*next_state)\n",
    "    \n",
    "    next_action_var = np.var(next_action.detach().cpu().numpy()).mean()\n",
    "    next_action_std = np.std(next_action.detach().cpu().numpy()).mean()\n",
    "    next_action_mean = np.mean(next_action.detach().cpu().numpy()).mean()\n",
    "    next_action_cov = np.cov(next_action.detach().cpu().numpy())\n",
    "    next_action_emb = torch.cat((next_action[0].unsqueeze(0), next_state[0][0]))\n",
    "    \n",
    "    target_value   = target_value_net(enc_next_state, next_action.detach())\n",
    "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "    \n",
    "    value = value_net(enc_state, action)\n",
    "    \n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "    losses = [value_loss.item(), policy_loss.item(), current_action_var,\n",
    "              current_action_std, current_action_mean, current_action_cov,\n",
    "              next_action_var, next_action_std, next_action_mean, next_action_cov,\n",
    "              current_action_emb, next_action_emb\n",
    "             ]\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    test_batch = []\n",
    "    while 1:\n",
    "        for user, df in test_ratings.groupby(level=0):\n",
    "            size = max(len(df) - frame_size, 0)\n",
    "            for idx in range(0, size):\n",
    "                test_batch.append(get_minibatch(df, idx))\n",
    "                if len(test_batch) >= batch_size:\n",
    "                    # train here\n",
    "                    test_batch = prepare_batch(test_batch)\n",
    "                    state, action, reward, next_state, done = test_batch\n",
    "\n",
    "                    enc_state, current_action = target_policy_net(*state)\n",
    "                    policy_loss = target_value_net(enc_state, current_action)\n",
    "                    policy_loss = policy_loss.mean()\n",
    "                    # mean estimated reccomended rating\n",
    "                    writer.add_scalar('metrics/MERR',\n",
    "                                      policy_loss, n_iter)\n",
    "                    test_batch = []\n",
    "                    yield policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net  = Critic(256, 128, 320).to(cuda)\n",
    "policy_net = Actor(256, 128, 192).to(cuda)\n",
    "\n",
    "target_value_net  = Critic(256, 128, 320).to(cuda)\n",
    "target_policy_net = Actor(256,128, 192).to(cuda)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "value_lr  = 10e-5\n",
    "policy_lr = 10e-6\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "value_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter('../runs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bdfa62cf324261ab1d20d23fd85a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138393), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_iter = 1\n",
    "test_every = 500\n",
    "test_gen = run_tests()\n",
    "\n",
    "# https://stackoverflow.com/questions/38543850/tensorflow-how-to-display-custom-images\n",
    "# -in-tensorboard-e-g-matplotlib-plots\n",
    "def gen_plot():\n",
    "    \"\"\"Create a pyplot plot and save to buffer.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot([1, 2])\n",
    "    plt.title(\"test\")\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    return buf\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    watched_infos = []\n",
    "    watched_rating = []\n",
    "    chosen_movie = []\n",
    "    chosen_rating = []\n",
    "    next_infos = []\n",
    "    next_rating = []\n",
    "    done = []\n",
    "    \n",
    "    for b in batch:\n",
    "        watched_infos.append(b[0][0])\n",
    "        watched_rating.append(torch.from_numpy(b[0][1]))\n",
    "        chosen_movie.append(b[1])\n",
    "        chosen_rating.append(b[2])\n",
    "        next_infos.append(b[3][0])\n",
    "        next_rating.append(torch.from_numpy(b[3][1]))\n",
    "        done.append(b[4])\n",
    "                    \n",
    "    watched_infos = torch.stack(watched_infos).to(cuda)\n",
    "    watched_rating = torch.stack(watched_rating).float().to(cuda)\n",
    "    chosen_movie = torch.stack(chosen_movie).to(cuda)\n",
    "    chosen_rating = torch.tensor(chosen_rating).to(cuda)\n",
    "    next_infos = torch.stack(next_infos).to(cuda)\n",
    "    next_rating = torch.stack(next_rating).float().to(cuda)\n",
    "    done = torch.tensor(done).float().to(cuda)\n",
    "    \n",
    "    return (watched_infos, watched_rating), chosen_movie, chosen_rating, \\\n",
    "           (next_infos, next_rating), done\n",
    "\n",
    "def get_minibatch(df, idx):\n",
    "    user_ratings = df[idx:frame_size + idx + 1]\n",
    "    user_ratings = user_ratings[[\"movieId\", \"rating\"]].values\n",
    "\n",
    "    chosen_movie = user_ratings[:, 0][-1] \n",
    "    chosen_movie = movies[chosen_movie] # action\n",
    "    chosen_rating = user_ratings[:, 1][-1] # reward\n",
    "    films_watched = user_ratings[:, 0][:-1]\n",
    "    watched_rating = user_ratings[:, 1][:-1] # state\n",
    "    watched_infos = [movies[i] for i in films_watched] # state\n",
    "    watched_infos = torch.stack(watched_infos)\n",
    "    next_infos = torch.cat((watched_infos[1:], chosen_movie.unsqueeze(0)), 0)\n",
    "    next_rating = watched_rating[1:].tolist()\n",
    "    next_rating.append(chosen_rating)\n",
    "    next_rating = np.array(next_rating)\n",
    "\n",
    "    # state action reward next_state done\n",
    "    return [(watched_infos, watched_rating), chosen_movie, chosen_rating,\n",
    "                  (next_infos, next_rating), idx + 1 == size]\n",
    "\n",
    "\n",
    "batch_bar = tqdm(total=len(train_users))\n",
    "batch = []\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "for user, df in train_ratings.groupby(level=0):\n",
    "    batch_bar.update(1)\n",
    "    size = max(len(df) - frame_size, 0)\n",
    "    for idx in range(0, size):\n",
    "        if np.random.rand() < 0.8:  # intake percents\n",
    "            continue\n",
    "        batch.append(get_minibatch(df, idx))\n",
    "        if len(batch) >= batch_size:\n",
    "            # train here\n",
    "            batch = prepare_batch(batch)\n",
    "            # losses:\n",
    "            # value_loss, policy_loss, current_action_var, current_action_std,\n",
    "            # current_action_mean, current_action_cov, next_action_var, next_action_std\n",
    "            # next_action_mean, next_action_cov\n",
    "            losses = ddpg_update(batch)\n",
    "            writer.add_scalar('loss/value', losses[0], n_iter)\n",
    "            writer.add_scalar('loss/policy', losses[1], n_iter)\n",
    "            writer.add_scalar('metrics_train/variance', losses[2], n_iter)\n",
    "            writer.add_scalar('metrics_train/std', losses[3], n_iter)\n",
    "            writer.add_scalar('metrics_train/mean', losses[4], n_iter)\n",
    "            writer.add_image('metrics_train/covariance', losses[5], n_iter, dataformats=\"HW\")\n",
    "            writer.add_scalar('metrics_train_target/variance', losses[6], n_iter)\n",
    "            writer.add_scalar('metrics_train_target/std', losses[7], n_iter)\n",
    "            writer.add_scalar('metrics_train_target/mean', losses[8], n_iter)\n",
    "            writer.add_image('metrics_train_target/covariance', losses[9], n_iter, dataformats=\"HW\")\n",
    "            emb_meta = ['generated'] + ['watched' for i in range(10)]\n",
    "            writer.add_embedding(losses[10], tag='metrics_train ' + str(n_iter), metadata=emb_meta)\n",
    "            writer.add_embedding(losses[11], tag='metrics_train_train ' + str(n_iter), metadata=emb_meta)\n",
    "            \n",
    "            if n_iter % test_every == 0:\n",
    "                next(test_gen)\n",
    "            n_iter += 1\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_value_net.state_dict(), \"../models/value.pt\")\n",
    "torch.save(target_policy_net.state_dict(), \"../models/policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_embedding('metrics_train', losses[10], n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = run_tests()\n",
    "for i in range(100):\n",
    "    next(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ratings.loc[138394]['movieId'].to_list()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
